{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYSNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "We recommend Anaconda distribution for python. We describe how you can set up a deep learning, data science environment that you can analyze cosmological datasets. We conda to install packages, and recommend you to use conda over pip. The installation can be divided into three steps:<br/>\n",
    "1. Install/update Conda <br/>\n",
    "2. Install Pytorch <br/>\n",
    "3. Install miscellaneous packages <br/>\n",
    "4. Install _SYSNet_ <br/>\n",
    "Throughout this note, we use '$>' to denote the commands that ought to be executed in the terminal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Conda\n",
    "First, you should check whether or not you have Conda installed on your system. Use the commandd `$> which conda` to see if conda is installed in the system. If not, please follow the instructions below to install conda:<br/> \n",
    "1.a Visit https://docs.conda.io/projects/conda/en/latest/index.html <br/>(or https://docs.conda.io/en/latest/miniconda.html#linux-installers for linux) <br/>\n",
    "\n",
    "On Linux, we would execute the following commands:<br/>\n",
    "1.b `$> wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh` <br/>\n",
    "1.c `$> sha256sum Miniconda3-latest-Linux-x86_64.sh` <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pytorch\n",
    "We recommend to take a look at the Pytorch [website](pytorch.org) to learn more about the framework. The installation of Pytorch on GPU-available machines is different from CPU-only machines. For instance, to set up on the **Ohio State Cluster (OSC)**, you should execute the next two commands to load the CUDA library. For other supercomputers, e.g., NERSC, you may need to read the documentation to see how you can load the CUDA library. <br/>\n",
    "2.a `$> module spider cuda # on OSC` <br/>\n",
    "2.b `$> module load cuda/10.1.168 # on OSC`\n",
    "\n",
    "For all other devices, i.e., CPU only, you can skip steps 2.a and 2.b, and follow the following steps to create the conda environment (e.g., called _sysnet_):<br/>\n",
    "2.c `$> conda create -n sysnet python=3.8 scikit-learn`<br/>\n",
    "\n",
    "Once your environment is created, you must activate it and use the appropriate Pytorch installation command to install Pytorch. For instance: <br/>\n",
    "2.d `$> conda activate sysnet` <br/>\n",
    "2.e `$> conda install pytorch torchvision -c pytorch`\n",
    "\n",
    "**Note**: The last step, for the OSC (see https://www.osc.edu/resources/available_software/software_list/cuda & https://www.osc.edu/supercomputing/batch-processing-at-osc/monitoring-and-managing-your-job), will be like:<br/>\n",
    "2.e `$> conda install pytorch torchvision cudatoolkit=10.1 -c pytorch # on OSC`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Miscellaneous\n",
    "After installation of `Pytorch`, execute the following commands to install the required packages:<br/>\n",
    "3.a `$> conda install git jupyter ipykernel ipython mpi4py`<br/>\n",
    "3.b `$> conda install -c conda-forge fitsio healpy absl-py pytables`\n",
    "\n",
    "Use the following command to add your env kernel (e.g., _sysnet_) to Jupyter:\n",
    "\n",
    "3.c `$> python -m ipykernel install --user --name=sysnet --display-name \"python (sysnet)\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. SYSNet\n",
    "Currently _SYSNet_ is under development and we have not made it pip installable. The only way to set it up is to clone the git repository. You should go to a desired directory (e.g. 'test' under the home directory):\n",
    "```\n",
    "$> cd $HOME\n",
    "$> mkdir test\n",
    "$> cd test\n",
    "```\n",
    "Then, you will clone the repo:<br/>\n",
    "`$> git clone https://github.com/mehdirezaie/sysnetdev.git` <br/>\n",
    "\n",
    "\n",
    "After cloning, we should pull from master branch to make sure the local repo is updated. To this end, go to the root directory of the sysnet software and pull from origin master: <br/>\n",
    "```\n",
    "$> cd sysnetdev\n",
    "$> git pull origin master\n",
    "```\n",
    "Then, insert the absolute path to SYSNET (or sysnetdev directory) to the environment variable `PYTHONPATH`:<br/>\n",
    "```\n",
    "$> export PYTHONPATH=/Users/mehdi/test/sysnetdev:${PYTHONPATH}\n",
    "```\n",
    "Congratulations! you have successfully set up _SYSNet_. You should add this line to `${HOME}/.bashrc` or `${HOME}/.bash_profile`, so everytime your system reboots, the environment variable is set. This is another hack to use the pipeline in Jupyter Notebook:<br/>\n",
    "```\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/mehdi/test/sysnetdev')\n",
    "```\n",
    "#### Test installation\n",
    "Navigate to the 'scripts' directory, and run the script app.py:<br/>\n",
    "```\n",
    "$> cd scripts\n",
    "$> python app.py -ax {0..17}\n",
    "```\n",
    "The last command will train the network for one epoch. Use `python app.py --help` to seek help for the full command line interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SYSNet in a Jupyter notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import sys \n",
    "sys.path.append('/Users/mehdi/github/sysnetdev') # add the path to SYSNet\n",
    "import sysnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = sysnet.sources.io.Config('../scripts/config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.restore_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "logging in ../output/model_test/train.log\n# --- inputs params ---\ninput_path: ../input/eBOSS.ELG.NGC.DR7.table.fits\noutput_path: ../output/model_test\nrestore_model: None\nbatch_size: 4098\nnepochs: 2\nnchains: 1\nfind_lr: False\nfind_structure: False\nfind_l1: False\ndo_kfold: False\nnormalization: z-score\nmodel: dnn\noptim: adamw\naxes: [0, 1, 2]\ndo_rfe: False\neta_min: 1e-05\nlearning_rate: 0.001\nnn_structure: [4, 20]\nl1_alpha: -1.0\nloss: mse\nloss_kwargs: {'reduction': 'sum'}\noptim_kwargs: {'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}\ndevice: cpu\npipeline initialized in 0.260 s\ndata loaded in 1.013 sec\n# running pipeline ...\n# training and evaluation\npartition_0 with (4, 20, 3, 1)\nbase_train_loss: 0.140\nbase_valid_loss: 0.142\nbase_test_loss: 0.144\n# running training and evaluation with seed: 2664485226\n"
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "File doesn't exist ../output/model_test/model_0_2664485226/None.pth.tar",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-cecc41fa2f4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# modeling (feature selection and regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msysnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYSNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/github/sysnetdev/sysnet/lab.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__tune_hyperparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             self.__train_and_eval_chains(\n\u001b[0m\u001b[1;32m    181\u001b[0m                 dataloaders, nn_structure, partition_id, stats)  # for 'nchains' times\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/sysnetdev/sysnet/lab.py\u001b[0m in \u001b[0;36m__train_and_eval_chains\u001b[0;34m(self, dataloaders, nn_structure, partition_id, stats)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 f'# running training and evaluation with seed: {seed}')\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             train_val_losses = self.__train(\n\u001b[0m\u001b[1;32m    226\u001b[0m                 dataloaders, nn_structure, seed, partition_id, stats)\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/sysnetdev/sysnet/lab.py\u001b[0m in \u001b[0;36m__train\u001b[0;34m(self, dataloaders, nn_structure, seed, partition_id, stats)\u001b[0m\n\u001b[1;32m    267\u001b[0m                       verbose=True, l1_alpha=self.config.l1_alpha)\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         losses = src.train_and_eval(model, optimizer, loss_fn, dataloaders, params,\n\u001b[0m\u001b[1;32m    270\u001b[0m                                     \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                                     restore_model=self.config.restore_model, return_losses=True)\n",
      "\u001b[0;32m~/github/sysnetdev/sysnet/sources/train.py\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(model, optimizer, loss_fn, dataloaders, params, checkpoint_path, scheduler, restore_model, return_losses)\u001b[0m\n\u001b[1;32m    147\u001b[0m             checkpoint_path, restore_model + '.pth.tar')\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m#logging.info(f\"Restoring parameters from {restore_path}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestore_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mbest_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'best_val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mbest_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/sysnetdev/sysnet/sources/io.py\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(checkpoint, model, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \"\"\"\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"File doesn't exist {checkpoint}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: File doesn't exist ../output/model_test/model_0_2664485226/None.pth.tar"
     ]
    }
   ],
   "source": [
    "# modeling (feature selection and regression\n",
    "pipeline = sysnet.SYSNet(config)\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SYSNet (add cross reference) <br/>\n",
    "    1. init <br/>\n",
    "        1.a initialize logger <br/>\n",
    "        1.b initialize loss functiom (L108) <br/>\n",
    "        1.c initialize model (Regression or Poisson Regression) <br/>\n",
    "        1.d initialize result collector <br/>\n",
    "        1.e initialize optimizer (e.g., AdamW or SGD) <br/>\n",
    "        1.f set the device (CPU or GPU) <br/>\n",
    "        1.g initialize the data loader (with or without K-fold partitioning) <br/>\n",
    "        1.h set the paths to the outputs (NN-prediction, metrics, best model weights, etc)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  make a working Jupyter example\n",
    "    - we just need to provide "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next meeting agenda\n",
    "```\n",
    "I. Run without L1\n",
    " Tune NN structure\n",
    " Tune Learning Rate Tuning\n",
    "\n",
    "II. Run with L1\n",
    " Tune L1 Scale\n",
    " Run with L1\n",
    " Compare with I\n",
    "\n",
    "III. Run with L1+L2\n",
    " Tune L1 and L2 scales\n",
    " Run with L1+L2\n",
    " compare with I & II\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (sysnet)",
   "language": "python",
   "name": "sysnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}