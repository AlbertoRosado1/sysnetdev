{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYSNet: Systematics Treatment with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SYSNet` was developed to tackle the problem of imaging systematic effects, e.g., Galactic dust, in galaxy survey data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "We recommend Conda for installation:\n",
    "```bash\n",
    "  conda create -q -n sysnet python=3.8 scikit-learn git jupyter ipykernel ipython mpi4py matplotlib\n",
    "  conda activate sysnet\n",
    "  conda install pytorch torchvision -c pytorch\n",
    "  conda install -c conda-forge fitsio healpy absl-py pytables pyyaml\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "#### Preprocessing\n",
    "The input data to `SYSNet` is a tabulated data of the following fields:\n",
    "1. `hpix`: (int) HEALPix index\n",
    "2. `label`: (float) number count of galaxies (or quasars) in pixel\n",
    "3. `fracgood`: (float) weight associated to pixel (the network output will be multiplied by this factor)\n",
    "4. `features`: (float) array holding imaging properties for pixel\n",
    "\n",
    "#### Neural Network regression\n",
    "The `SYSNet` software is called in this step to perform a regression analysis modeling the relationship between `label` and `features`.\n",
    "\n",
    "#### Postprocessing\n",
    "The `SYSNet` output will be used to assign appropriate _weights_ to galaxies to account for observational systematics.\n",
    "\n",
    "## Demo\n",
    "In the following, we aim to provide a demo of the regression and postprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys \n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "import fitsio as ft\n",
    "sys.path.append('/home/mehdi/github/sysnetdev') # 'Cause SYSNet is not installed yet\n",
    "from sysnet import SYSNet, Config, TrainedModel\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = ft.read('../input/eBOSS.ELG.NGC.DR7.table.fits')  # read tab. data\n",
    "\n",
    "for array in input_.dtype.descr:\n",
    "    print(array)\n",
    "     \n",
    "# for visualization    \n",
    "nside = 256\n",
    "ng = np.zeros(12*nside*nside)\n",
    "ng[:] = np.nan\n",
    "ng[input_['hpix']] = input_['label']    \n",
    "\n",
    "# Mollweide projection\n",
    "hp.mollview(ng, rot=-85, min=0.5, max=2.0,\n",
    "            title='Observed galaxy density [normalized]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config('../scripts/config.yaml')   # read config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what each keywork means:\n",
    "```\n",
    "'input_path': '../input/eBOSS.ELG.NGC.DR7.table.fits',    # path to the input file\n",
    " 'output_path': '../output/model_test',                   # dir path to the outputs\n",
    " 'restore_model': None,                                   # if you want to resume training?\n",
    " 'batch_size': 512,                                       # size of the mini-batch for training\n",
    " 'nepochs': 2,                                            # max number of training epochs\n",
    " 'nchains': 1,                                            # number of chains for the ensemble\n",
    " 'find_lr': False,                                        # run learning rate finder\n",
    " 'find_structure': False,                                 # run nn structure finder (brute force)\n",
    " 'find_l1': False,                                        # run L1 scale finder (brute force)\n",
    " 'do_kfold': False,                                       # perfom k-fold validation (k=5)\n",
    " 'do_tar': False,                                         # tar some of the output files \n",
    " 'snapshot_ensemble': False,                              # run snapshot ensemble, train one get M for free\n",
    " 'normalization': 'z-score',                              # normalization rule for the features\n",
    " 'model': 'dnn',                                          # name of the model, e.g., dnn or dnnp\n",
    " 'optim': 'adamw',                                        # name of the optimizer\n",
    " 'scheduler': 'cosann',                                   # name of the scheduler\n",
    " 'axes': [0, 1, 2],                                       # list of feature indices\n",
    " 'do_rfe': False,                                         # perform recursive feature elimination (ablation)\n",
    " 'eta_min': 1e-05,                                        # min learning rate\n",
    " 'learning_rate': 0.001,                                  # initial learning rate\n",
    " 'nn_structure': [4, 20],                                 # structure of the neural net (# layers, # units)     \n",
    " 'l1_alpha': -1.0,                                        # L1 scale, if < 0, it will be ignored\n",
    " 'loss': 'mse'                                            # name of the loss function, e.g., mse or pnll\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important hyper-parameter, The learning rate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# let's update some input arguments\n",
    "config.update('nepochs', 30)\n",
    "config.update('axes', [i for i in range(18)]) # num of features for this dataset\n",
    "config.update('batch_size', 4096)\n",
    "\n",
    "# run learning rate finder\n",
    "config.update('find_lr', True)\n",
    "pipeline = SYSNet(config) # perform regression\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the produce plot:\n",
    "![this image](../output/model_test/loss_vs_lr_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this plot, we should use the initial learning rate around 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# let's re-train the network with the best learning rate\n",
    "# monitor validation loss\n",
    "\n",
    "config.update('find_lr', False) # remember to turn this off\n",
    "config.update('learning_rate', 0.01)\n",
    "pipeline = SYSNet(config) # perform regression\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows the training and validation loss vs epoch.\n",
    "![this image](../output/model_test/model_0_2664485226/loss_model_0_2664485226.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model returns 0.144 for the test loss, while the neural network is able to yield a lower value, 0.130.The validation loss is also not showing any sign of over-fitting. \n",
    "\n",
    "The code outputs several files:\n",
    "1. `nn-weights.fits`: this file has healpix index and predicted galaxy count\n",
    "2. `metrics.npz`: training, validation, test loss and mean and std of features\n",
    "3. `best.pth.tar`: the best model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = TrainedModel('dnn', # name of the model\n",
    "                  '../output/model_test/model_0_2664485226/best.pth.tar', # best model part 0 seed 2664485226\n",
    "                  0, # partition of the model\n",
    "                  (4, 20), # structure of the network\n",
    "                  18) # num of input features\n",
    "\n",
    "hpix, npred = tm.forward('../output/model_test/metrics.npz',  # metrics, we need the mean and std of features\n",
    "                         '../input/eBOSS.ELG.NGC.DR7.table.fits') \n",
    "npred = npred / npred.mean()  # normalize\n",
    "npred = npred.clip(0.5, 2.0)  # avoid extreme predictions\n",
    "\n",
    "\n",
    "ng_ = np.zeros(12*256*256)\n",
    "ng_[:] = np.nan\n",
    "ng_[hpix] = npred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.mollview(ng, rot=-85, min=0.5, max=2.0,\n",
    "            title='Observed galaxy density [normalized]')\n",
    "\n",
    "hp.mollview(ng_, rot=-85, min=0.5, max=2.0,\n",
    "            title='Predicted galaxy density [normalized]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 30 training epochs, a single fold, and a single chain model, this is not a bad result. In practice, we use 5-fold validation (`do_kfold=True`) and train 20 chains (`nchains=20`) for70-300 epochs.\n",
    "\n",
    "If you have any questions, feel free to email me at mr095415@ohio.edu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (sysnet)",
   "language": "python",
   "name": "sysnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
